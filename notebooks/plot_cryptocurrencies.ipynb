{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark, Jupyter & Plotly \n",
    "\n",
    "## tl;dr\n",
    "\n",
    "From the menu, click `Cell -> Run All`. Scroll to the bottom. Wait patiently for the plot to appear. \n",
    "\n",
    "### Things to Know\n",
    "\n",
    "1. If you are new to notebooks and want to learn about them more, see [this article](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook)\n",
    "2. The `Help` section in the menu links to handy references\n",
    "3. This notebook is themed with a dark style so the programmer's eyes don't hurt. The `jupyterthemes` styling removes the top navigation bar. Don't be alarmed.\n",
    "4. If you want to learn more about Spark, see [A Gentle Introduction to Spark](http://go.databricks.com/gentle-intro-spark)\n",
    "\n",
    "### If You Have An Issue Running A Cell\n",
    "\n",
    "1. It's possible the package is not installed. You can navigate to the root path on the host (i.e. the url in your browser with out any `/`'s behind it) and then open the menu `New->Terminal` and use `pip install --user <package>` to install the proper package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat /etc/*-release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Notebook to Your Screen.\n",
    "\n",
    "The below cell will update the `css` to fit the notebook to the width of your browser window. This is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Bitcoin & Ethereum Pricing Data\n",
    "\n",
    "Try to not execute this often, as the API is rate limited. Just once is enough, as notebooks are stateful and the files will already be written to the file system after the first execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "btc_url = \"https://www.quandl.com/api/v3/datasets/BCHARTS/BITSTAMPUSD.csv?api_key=T_g99ExMU_4X_Bgss6Zx\"\n",
    "eth_url = \"https://etherscan.io/chart/etherprice?output=csv\"\n",
    "\n",
    "response = requests.get(btc_url)\n",
    "f_btc = open('bitcoin.csv', 'w')\n",
    "f_btc.write(response.text)\n",
    "f_btc.close()\n",
    "\n",
    "response = requests.get(eth_url)\n",
    "f_eth = open('ethereum.csv', 'w')\n",
    "f_eth.write(response.text)\n",
    "f_eth.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Spark Context with PySpark\n",
    "\n",
    "In case you are new to Spark (like me), you should know the core engine is written in Scala. The language binding to Python is known as PySpark. The Spark Context is shared session state for a program's interaction with the Spark cluster. The cluster resources can be used by 1 or more programs, and thus Spark supports more than one session. This spark deployed is local, but in the future this will updated with radanalytics.io drivers to spin up a cluster in OpenShift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Spark Dataframe for Bitcoin Prices\n",
    "\n",
    "You can think of a Dataframe like a table in a SQL database, composed of rows and columns. Here, we'll load the `csv` created earlier as a Dataframe, and then create a view of the data so we can query it with Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"bitcoin.csv\")\n",
    "btc_df.createOrReplaceTempView(\"bitcoin_prices\")\n",
    "\n",
    "dates = spark.sql(\"SELECT Date as d FROM bitcoin_prices\")\n",
    "opening = spark.sql(\"SELECT Close as o FROM bitcoin_prices\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Spark Dataframe for Ethereum Prices\n",
    "\n",
    "Much like the Bitcoin prices, but here we need to massage the data to take the same shape as the bitcoin data in order to plot it correctly. To this, we need to do a few transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df_eth = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"ethereum.csv\")\n",
    "\n",
    "def reformat_date(string): \n",
    "    tokens = string.split(\"/\")\n",
    "    date = \"{}-{}-{}\".format(tokens[2],tokens[0],tokens[1])\n",
    "    return date\n",
    "    \n",
    "\n",
    "df_eth2 = (df_eth.withColumnRenamed(\"Date(UTC)\", \"date\").\n",
    "    withColumnRenamed(\"UnixTimeStamp\", \"timestap\").\n",
    "    withColumnRenamed(\"Value\", \"value\"))\n",
    "\n",
    "df_eth3 = df_eth2.rdd.map( lambda r : \n",
    "                       Row( date = reformat_date(r[0]), \n",
    "                           timestamp = r[1],\n",
    "                           value = r[2])\n",
    "                      ).toDF()\n",
    "\n",
    "df_eth3.createOrReplaceTempView(\"ethereum_prices\")\n",
    "\n",
    "\n",
    "dates_eth = spark.sql(\"SELECT date as d FROM ethereum_prices\")\n",
    "values = spark.sql(\"SELECT value as v FROM ethereum_prices\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create The Plot with Plotly\n",
    "\n",
    "Spark does not yet have native ploting tools, so we need to take the transforms we need in Spark and export them back to Python Pandas, which despite the cute name, are just Python native representations of the `Dataframe` concept that integrate with Plotlty, our graphing library.\n",
    "\n",
    "Once the graph is created, be sure to zoom in to interesting areas by click and dragging a region. Then, double click to zoom back out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "trace_btc = go.Scatter(\n",
    "    x = dates.toPandas()['d'],\n",
    "    y = opening.toPandas()['o'],\n",
    "    name = \"bitcoin\"\n",
    ")\n",
    "trace_eth = go.Scatter(\n",
    "    x = dates_eth.toPandas()['d'],\n",
    "    y = values.toPandas()['v'],\n",
    "    name = \"etherium\"\n",
    ")\n",
    "\n",
    "data = [trace_eth, trace_btc]\n",
    "plotly.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
